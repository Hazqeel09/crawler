{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f9d1ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import jsonlines\n",
    "import ast\n",
    "import codecs\n",
    "import langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce25136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ajar article from 1/3/2018 - todays date 8/7/2023\n",
    "start_date = '2018/03/01'\n",
    "today = datetime.now()\n",
    "article_dates = pd.date_range(start_date,today).strftime(\"%Y/%m/%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c82b6498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1956/1956 [22:35<00:00,  1.44it/s]\n"
     ]
    }
   ],
   "source": [
    "hrefs = []\n",
    "\n",
    "for article_date in tqdm(article_dates):\n",
    "    while True:\n",
    "        try:\n",
    "            r = requests.get(f\"https://ajar.com.my/{article_date}/\")\n",
    "            if r.status_code == 200 or r.status_code == 404:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(1.3)\n",
    "\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    div = soup.find_all('h2', attrs = {\"class\":\"cs-entry__title\"})\n",
    "\n",
    "    if len(div) == 0:\n",
    "        continue\n",
    "        \n",
    "    for i in range(len(div)):\n",
    "        for j in range(len(div[i].find_all(\"a\"))):\n",
    "            hrefs.append(div[i].find_all(\"a\")[j].get(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09dca2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "642"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hrefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cc83f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "642"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(hrefs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c009cdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 642/642 [11:26<00:00,  1.07s/it]\n"
     ]
    }
   ],
   "source": [
    "for href in tqdm(hrefs):\n",
    "    while True:\n",
    "        try:\n",
    "            r = requests.get(href)\n",
    "            if r.status_code == 200:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(3.9)\n",
    "            \n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    title = soup.find(\"h1\", attrs = {\"class\":\"cs-entry__title cs-entry__title-line\"}).text.strip()\n",
    "    \n",
    "    div = soup.find_all('div', attrs = {\"class\":\"entry-content\"})\n",
    "    p = [p.text for p in div[0].find_all([\"p\",\"h2\"]) if len(p.text.split()) > 0]\n",
    "    \n",
    "    data = {'url': href, 'headline': title, 'content': p}\n",
    "    \n",
    "    with open('ajar-before.jsonl', 'a') as f:\n",
    "        json.dump(data, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fd09fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the input and output file paths\n",
    "input_file = \"ajar-before.jsonl\"\n",
    "output_file = \"ajar.jsonl\"\n",
    "\n",
    "langid.set_languages(['ms', 'en'])  #ISO 639-1 codes\n",
    "\n",
    "# Open the input and output files\n",
    "with jsonlines.open(input_file) as reader, jsonlines.open(output_file, mode='w') as writer:\n",
    "    # Iterate over each line in the input file\n",
    "    for line in reader:\n",
    "        # Check if the content field is empty ([] or {})\n",
    "        if line['content']:\n",
    "            # Write the non-empty line to the output file\n",
    "            decoded_list = []\n",
    "            for string in line['content']:\n",
    "                try:\n",
    "                    decoded_string = codecs.escape_decode(string)[0].decode('utf-8')\n",
    "                except ValueError:\n",
    "                    decoded_string = string\n",
    "                decoded_list.append(decoded_string)\n",
    "            line['content'] = decoded_list\n",
    "\n",
    "            writer.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5ca0e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
